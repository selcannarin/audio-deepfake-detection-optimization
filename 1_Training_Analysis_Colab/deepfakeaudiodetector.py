# -*- coding: utf-8 -*-
"""DeepfakeAudioDetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IiWpm3JHQrF-X_By6350hbM2DxUh0xuL
"""

# ============================================================
# ðŸ”„ RUNTIME SIFIRLAMA (Colab menÃ¼sÃ¼nden yap)
# ============================================================
# Runtime â†’ Disconnect and delete runtime â†’ Connect
# Sonra bu hÃ¼creyi Ã§alÄ±ÅŸtÄ±r:

!pip uninstall -y numpy torch torchvision torchaudio -q
!pip install numpy==2.0.2 -q
!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121 -q
!pip install librosa==0.10.1 soundfile==0.12.1 scikit-learn==1.5.2 -q
!pip install datasets==2.21.0 lightgbm==4.5.0 matplotlib seaborn tqdm -q

print("âœ… Kurulum tamamlandÄ±. Kernel'i yeniden baÅŸlat (Runtime â†’ Restart runtime)")

"""
SECTION 0: Configuration and Environment Setup
Deepfake Audio Detection - Optimization Theory Project
ASVspoof 2019 LA Dataset

"""

import os
import gc
import json
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import librosa
import soundfile as sf
from tqdm.auto import tqdm
from collections import Counter

# Dataset
from datasets import load_dataset

# Google Colab
from google.colab import drive

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score,
    classification_report, confusion_matrix
)

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

# LightGBM
from lightgbm import LGBMClassifier
import joblib

print("="*80)
print("SECTION 0: Configuration and Environment Setup")
print("="*80)

# ============================================================
# REPRODUCIBILITY: Fixed Random Seeds
# ============================================================

def set_seed(seed=42):
    """
    Fix all random seeds for reproducibility
    Critical for optimizer comparison experiments
    """
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

SEED = 42
set_seed(SEED)

print(f"\nRandom seed fixed: {SEED}")
print("All experiments are now reproducible")

# ============================================================
# DEVICE CONFIGURATION
# ============================================================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"\nDevice Configuration:")
print(f"  PyTorch version: {torch.__version__}")
print(f"  CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  CUDA version: {torch.version.cuda}")
print(f"  Using device: {device}")

# ============================================================
# DIRECTORY STRUCTURE
# ============================================================

drive.mount('/content/drive', force_remount=False)

BASE_DIR = "/content/drive/MyDrive/DeepfakeOptimization_Clean"

DIRS = {
    'models': os.path.join(BASE_DIR, 'models'),
    'reports': os.path.join(BASE_DIR, 'reports'),
    'figures': os.path.join(BASE_DIR, 'figures'),
    'artifacts': os.path.join(BASE_DIR, 'artifacts')
}

for name, path in DIRS.items():
    os.makedirs(path, exist_ok=True)

print(f"\nDirectory Structure:")
print(f"  Base: {BASE_DIR}")
for name, path in DIRS.items():
    print(f"  {name}: ./{os.path.basename(path)}/")

# ============================================================
# EXPERIMENT CONFIGURATION
# ============================================================

CONFIG = {
    'seed': SEED,
    'device': str(device),

    # Dataset parameters
    'dataset': 'ASVspoof_2019_LA',
    'n_samples_per_class': 1500,
    'test_size': 0.2,

    # Audio parameters
    'sr': 16000,
    'duration': 2.0,

    # Feature parameters
    'n_mfcc': 20,
    'n_lfcc': 20,
    'n_lps': 20,
    'n_contrast_bands': 6,
    'mpe_scales': [1, 2, 4, 8, 16],
    'mpe_orders': [3, 4, 5, 6],

    # Training parameters - OPTIMIZER FOCUS
    'batch_size': None,  # Full batch for optimizer analysis
    'epochs_cnn': 50,
    'epochs_seresnet': 50,

    # Optimizer configurations - THIS IS THE CORE
    'optimizers': {
        'sgd': {
            'lr': 0.01,
            'momentum': 0.9,
            'weight_decay': 0.0001,
            'nesterov': True
        },
        'adam': {
            'lr': 0.001,
            'betas': (0.9, 0.999),
            'eps': 1e-8,
            'weight_decay': 0.01
        },
        'adamw': {
            'lr': 0.001,
            'betas': (0.9, 0.999),
            'eps': 1e-8,
            'weight_decay': 0.01
        },
        'rmsprop': {
            'lr': 0.001,
            'alpha': 0.99,
            'eps': 1e-8,
            'weight_decay': 0.01
        }
    },

    # Model parameters
    'dropout_rate': 0.5,
    'focal_alpha': 0.25,
    'focal_gamma': 2.0
}

# Save configuration
config_path = os.path.join(DIRS['reports'], 'config.json')
with open(config_path, 'w') as f:
    json.dump(CONFIG, f, indent=2)

print(f"\nConfiguration saved: {config_path}")

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def save_figure(fig, filename, dpi=150):
    """Save matplotlib figure"""
    path = os.path.join(DIRS['figures'], filename)
    fig.savefig(path, dpi=dpi, bbox_inches='tight')
    plt.close(fig)
    return path

def save_model(model, filename):
    """Save PyTorch model or sklearn model"""
    path = os.path.join(DIRS['models'], filename)
    if isinstance(model, nn.Module):
        torch.save(model.state_dict(), path)
    else:
        joblib.dump(model, path)
    return path

def save_report(data, filename):
    """Save JSON report"""
    path = os.path.join(DIRS['reports'], filename)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    return path

def track_gradient_norm(model):
    """
    Track gradient norms for optimizer analysis
    Critical for understanding optimizer behavior
    """
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    return total_norm

print("\nHelper functions defined")

print("\n" + "="*80)
print("SECTION 0 COMPLETE")
print("="*80)
print("\nFor Report:")
print("  'All experiments conducted with fixed random seeds (seed=42)")
print("  for reproducibility. Optimizer configurations explicitly defined")
print("  for comparative analysis.'")
print("\n" + "="*80)

"""
SECTION 1: Data Ingestion
Load ASVspoof 2019 LA dataset with balanced sampling

"""

print("\n" + "="*80)
print("SECTION 1: Data Ingestion")
print("="*80)

# ============================================================
# DATASET LOADING
# ============================================================

print("\nLoading ASVspoof 2019 LA Dataset (Streaming Mode)...")

dataset = load_dataset("Bisher/ASVspoof_2019_LA", split="train", streaming=True)

print("Dataset loaded successfully")
print(f"  Dataset: ASVspoof 2019 LA (Logical Access)")
print(f"  Mode: Streaming (memory efficient)")

# ============================================================
# BALANCED SAMPLING
# ============================================================

N_SAMPLES_PER_CLASS = CONFIG['n_samples_per_class']

print(f"\nCollecting samples...")
print(f"  Target: {N_SAMPLES_PER_CLASS} bonafide + {N_SAMPLES_PER_CLASS} spoof")

audio_data = []
labels = []
bonafide_count = 0
spoof_count = 0

pbar = tqdm(total=N_SAMPLES_PER_CLASS*2, desc="Collecting", unit="sample")

for example in dataset:
    try:
        label = example["key"]  # 0: bonafide, 1: spoof

        # Balance check
        if label == 0 and bonafide_count >= N_SAMPLES_PER_CLASS:
            continue
        if label == 1 and spoof_count >= N_SAMPLES_PER_CLASS:
            continue

        # Store audio info
        audio_data.append({
            'audio': example["audio"]["array"],
            'sr': example["audio"]["sampling_rate"],
            'label': label
        })
        labels.append(label)

        if label == 0:
            bonafide_count += 1
        else:
            spoof_count += 1

        pbar.update(1)

        # Early exit
        if bonafide_count >= N_SAMPLES_PER_CLASS and spoof_count >= N_SAMPLES_PER_CLASS:
            break

    except Exception as e:
        continue

pbar.close()

# ============================================================
# CLASS DISTRIBUTION ANALYSIS
# ============================================================

labels = np.array(labels)
class_distribution = Counter(labels)

print(f"\nClass Distribution:")
print(f"  Bonafide (0): {class_distribution[0]} samples")
print(f"  Spoof (1): {class_distribution[1]} samples")
print(f"  Total: {len(labels)} samples")

# Balance ratio
balance_ratio = min(class_distribution.values()) / max(class_distribution.values())
print(f"\n  Balance ratio: {balance_ratio:.3f}")

if balance_ratio >= 0.95:
    print("  Dataset is well-balanced")
elif balance_ratio >= 0.80:
    print("  Dataset is moderately balanced")
else:
    print("  Dataset is imbalanced - consider resampling")

# ============================================================
# SAVE DISTRIBUTION REPORT
# ============================================================

distribution_report = {
    'total_samples': int(len(labels)),
    'bonafide_count': int(class_distribution[0]),
    'spoof_count': int(class_distribution[1]),
    'balance_ratio': float(balance_ratio),
    'target_per_class': N_SAMPLES_PER_CLASS,
    'achieved_balance': 'Yes' if balance_ratio >= 0.95 else 'No'
}

report_path = save_report(distribution_report, 'class_distribution.json')
print(f"\nReport saved: {report_path}")

# ============================================================
# VISUALIZATION
# ============================================================

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Pie chart
colors = ['#2ecc71', '#e74c3c']
axes[0].pie([class_distribution[0], class_distribution[1]],
            labels=['Bonafide', 'Spoof'],
            colors=colors,
            autopct='%1.1f%%',
            startangle=90)
axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')

# Bar chart
axes[1].bar(['Bonafide', 'Spoof'],
            [class_distribution[0], class_distribution[1]],
            color=colors,
            alpha=0.7)
axes[1].set_ylabel('Number of Samples')
axes[1].set_title('Sample Counts', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

# Add count labels
for i, (label, count) in enumerate(class_distribution.items()):
    axes[1].text(i, count + 20, str(count), ha='center', fontweight='bold')

plt.suptitle('ASVspoof 2019 LA - Data Ingestion Analysis',
             fontsize=16, fontweight='bold')

fig_path = save_figure(fig, 'class_distribution.png')
print(f"Figure saved: {fig_path}")

# ============================================================
# STORE FOR NEXT STAGE
# ============================================================

print(f"\nData ingestion completed")
print(f"  Collected: {len(audio_data)} audio samples")
print(f"  Ready for preprocessing")

print("\n" + "="*80)
print("SECTION 1 COMPLETE")
print("="*80)
print("\nFor Report:")
print("  'Dataset consists of 3,000 balanced samples (1,500 bonafide,")
print("  1,500 spoof) from ASVspoof 2019 LA training set.'")
print("\n" + "="*80)

"""
SECTION 2: Audio Preprocessing
Standardize audio length and normalize amplitude

"""

print("\n" + "="*80)
print("SECTION 2: Audio Preprocessing")
print("="*80)

SR = CONFIG['sr']
DURATION = CONFIG['duration']
SAMPLES = int(SR * DURATION)

print(f"\nPreprocessing Parameters:")
print(f"  Target sample rate: {SR} Hz")
print(f"  Fixed duration: {DURATION} seconds")
print(f"  Fixed length: {SAMPLES} samples")

# ============================================================
# ANALYZE ORIGINAL LENGTHS
# ============================================================

print("\nAnalyzing original audio lengths...")

original_lengths = []
original_durations = []

for item in tqdm(audio_data, desc="Analyzing", unit="audio"):
    audio = item['audio']
    sr_orig = item['sr']

    original_lengths.append(len(audio))
    original_durations.append(len(audio) / sr_orig)

original_lengths = np.array(original_lengths)
original_durations = np.array(original_durations)

print(f"\nOriginal Audio Statistics:")
print(f"  Mean duration: {original_durations.mean():.2f}s")
print(f"  Std duration: {original_durations.std():.2f}s")
print(f"  Min duration: {original_durations.min():.2f}s")
print(f"  Max duration: {original_durations.max():.2f}s")
print(f"  Median duration: {np.median(original_durations):.2f}s")

# ============================================================
# PREPROCESSING FUNCTION
# ============================================================

def preprocess_audio(audio, sr_orig, target_sr=SR, target_samples=SAMPLES):
    """
    Preprocess audio: resample + normalize length

    Args:
        audio: Raw audio array
        sr_orig: Original sample rate
        target_sr: Target sample rate
        target_samples: Target number of samples

    Returns:
        preprocessed: Fixed-length audio at target SR
    """
    # Resample if needed
    if sr_orig != target_sr:
        audio = librosa.resample(audio, orig_sr=sr_orig, target_sr=target_sr)

    # Normalize length
    if len(audio) < target_samples:
        # Pad with zeros
        audio = np.pad(audio, (0, target_samples - len(audio)), mode='constant')
    else:
        # Truncate
        audio = audio[:target_samples]

    return audio

# ============================================================
# APPLY PREPROCESSING
# ============================================================

print("\nPreprocessing audio samples...")

preprocessed_audio = []

for item in tqdm(audio_data, desc="Preprocessing", unit="audio"):
    audio = preprocess_audio(item['audio'], item['sr'])
    preprocessed_audio.append(audio)

preprocessed_audio = np.array(preprocessed_audio, dtype=np.float32)

print(f"\nPreprocessing completed")
print(f"  Shape: {preprocessed_audio.shape}")
print(f"  All samples: {SAMPLES} length ({DURATION}s @ {SR}Hz)")
print(f"  Memory: {preprocessed_audio.nbytes / 1024**2:.2f} MB")

# Verify all same length
preprocessed_lengths = [len(audio) for audio in preprocessed_audio]
assert all(length == SAMPLES for length in preprocessed_lengths), "Length mismatch!"

print(f"\nVerification:")
print(f"  All samples standardized to {SAMPLES} samples")

# ============================================================
# VISUALIZATION
# ============================================================

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Before preprocessing
axes[0].hist(original_durations, bins=50, alpha=0.7, color='orange', edgecolor='black')
axes[0].axvline(DURATION, color='red', linestyle='--', linewidth=2, label=f'Target: {DURATION}s')
axes[0].set_xlabel('Duration (seconds)')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Audio Length Distribution - Before Preprocessing', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# After preprocessing
preprocessed_durations = [length / SR for length in preprocessed_lengths]
axes[1].hist(preprocessed_durations, bins=1, alpha=0.7, color='green', edgecolor='black')
axes[1].set_xlabel('Duration (seconds)')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Audio Length Distribution - After Preprocessing', fontsize=12, fontweight='bold')
axes[1].set_xlim([DURATION - 0.1, DURATION + 0.1])
axes[1].grid(True, alpha=0.3)

plt.suptitle('Audio Preprocessing - Length Normalization', fontsize=16, fontweight='bold')

fig_path = save_figure(fig, 'audio_length_distribution.png')
print(f"\nFigure saved: {fig_path}")

print("\n" + "="*80)
print("SECTION 2 COMPLETE")
print("="*80)

"""
SECTION 3: Robust Feature Extraction
Extract multi-scale features for deepfake detection

Feature Set (154-D):
- LPS (Log Power Spectrum): 40 features - codec resistant
- LFCC (Linear Frequency Cepstral Coefficients): 40 features - linear frequency
- Spectral Contrast: 14 features - spoof discrimination
- MPE (Multi-scale Permutation Entropy): 20 features - temporal complexity
- MFCC (Mel-Frequency Cepstral Coefficients): 40 features - baseline

Total: 154 features per sample
"""

print("\n" + "="*80)
print("SECTION 3: Robust Feature Extraction")
print("="*80)

N_MFCC = CONFIG['n_mfcc']
N_LFCC = CONFIG['n_lfcc']
N_LPS = CONFIG['n_lps']
N_CONTRAST_BANDS = CONFIG['n_contrast_bands']
MPE_SCALES = CONFIG['mpe_scales']
MPE_ORDERS = CONFIG['mpe_orders']

print(f"\nFeature Configuration:")
print(f"  LPS: {N_LPS*2} features (mean + std)")
print(f"  LFCC: {N_LFCC*2} features (mean + std)")
print(f"  Spectral Contrast: {(N_CONTRAST_BANDS+1)*2} features (mean + std)")
print(f"  MPE: {len(MPE_SCALES)*len(MPE_ORDERS)} features")
print(f"  MFCC: {N_MFCC*2} features (mean + std)")
print(f"\n  Total: {N_LPS*2 + N_LFCC*2 + (N_CONTRAST_BANDS+1)*2 + len(MPE_SCALES)*len(MPE_ORDERS) + N_MFCC*2} features per sample")

# ============================================================
# LPS (LOG POWER SPECTRUM)
# ============================================================

def extract_lps(audio, sr=SR, n_fft=512, hop_length=160, n_lps=N_LPS):
    """
    Log Power Spectrum - codec-resistant feature
    Superior to MFCC for degraded audio

    Args:
        audio: Audio signal
        sr: Sample rate
        n_fft: FFT size
        hop_length: Hop length
        n_lps: Number of LPS bins

    Returns:
        lps_features: LPS mean + std (40 features)
    """
    # STFT
    S = np.abs(librosa.stft(audio, n_fft=n_fft, hop_length=hop_length))

    # Power spectrum
    P = S ** 2

    # Log with numerical stability
    LPS = np.log(P + 1e-10)

    # Calculate statistics per frequency bin
    lps_mean_per_freq = LPS.mean(axis=1)
    lps_std_per_freq = LPS.std(axis=1)

    # Process features to target dimension
    def process_freq_features(features_per_freq, target_n_lps):
        if features_per_freq.shape[0] > target_n_lps:
            # Downsample
            step = max(1, features_per_freq.shape[0] // target_n_lps)
            processed_features = np.array([
                features_per_freq[i*step:(i+1)*step].mean()
                for i in range(target_n_lps)
            ])
        elif features_per_freq.shape[0] < target_n_lps:
            # Pad
            processed_features = np.pad(
                features_per_freq,
                (0, target_n_lps - features_per_freq.shape[0]),
                mode='constant'
            )
        else:
            processed_features = features_per_freq
        return processed_features

    lps_mean = process_freq_features(lps_mean_per_freq, n_lps)
    lps_std = process_freq_features(lps_std_per_freq, n_lps)

    return np.concatenate([lps_mean, lps_std]).astype(np.float32)

# ============================================================
# SPECTRAL CONTRAST
# ============================================================

def extract_spectral_contrast(audio, sr=SR, n_bands=N_CONTRAST_BANDS):
    """
    Spectral Contrast - strong for bonafide vs spoof separation
    Deepfakes typically have lower spectral contrast

    Args:
        audio: Audio signal
        sr: Sample rate
        n_bands: Number of frequency bands

    Returns:
        contrast_features: Contrast mean + std (14 features)
    """
    contrast = librosa.feature.spectral_contrast(
        y=audio,
        sr=sr,
        n_bands=n_bands
    )

    contrast_mean = contrast.mean(axis=1)
    contrast_std = contrast.std(axis=1)

    return np.concatenate([contrast_mean, contrast_std]).astype(np.float32)

# ============================================================
# MPE (MULTI-SCALE PERMUTATION ENTROPY)
# ============================================================

def permutation_entropy(signal, order=3, delay=1):
    """
    Calculate Permutation Entropy
    Measures complexity of time series

    PE = -sum(p(Ï€) * log2(p(Ï€)))
    where Ï€ are permutation patterns

    Args:
        signal: 1D time series
        order: Embedding dimension (pattern length)
        delay: Time delay

    Returns:
        pe: Permutation entropy value (0 = regular, high = chaotic)
    """
    n = len(signal)
    if n < delay * (order - 1) + 1:
        return 0.0

    permutations = []

    # Generate permutation patterns
    for i in range(n - delay * (order - 1)):
        indices = [i + j * delay for j in range(order)]
        pattern = signal[indices]

        # Get rank order (permutation)
        perm = tuple(np.argsort(pattern))
        permutations.append(perm)

    if len(permutations) == 0:
        return 0.0

    # Count frequencies
    perm_counts = Counter(permutations)

    # Calculate probabilities
    total = len(permutations)
    probabilities = np.array([count / total for count in perm_counts.values()])

    # Shannon entropy
    pe = -np.sum(probabilities * np.log2(probabilities + 1e-10))

    return pe

def extract_mpe_features(audio, sr=SR, scales=MPE_SCALES, orders=MPE_ORDERS):
    """
    Multi-scale Permutation Entropy (MPE)
    Captures temporal complexity at multiple time scales

    Args:
        audio: Audio signal
        sr: Sample rate
        scales: List of time scales
        orders: List of embedding dimensions

    Returns:
        mpe_features: Array of MPE values (scales x orders)
    """
    mpe_features = []

    for order in orders:
        for scale in scales:
            # Coarse-grain signal
            if scale == 1:
                coarse_signal = audio
            else:
                n_samples = len(audio) // scale
                if n_samples < order:
                    mpe_features.append(0.0)
                    continue
                coarse_signal = audio[:n_samples * scale].reshape(n_samples, scale).mean(axis=1)

            # Calculate PE
            pe = permutation_entropy(coarse_signal, order=order, delay=1)
            mpe_features.append(pe)

    return np.array(mpe_features, dtype=np.float32)

# ============================================================
# UNIFIED FEATURE EXTRACTION
# ============================================================

def extract_all_robust_features(audio, sr=SR):
    """
    Extract complete robust feature set

    Args:
        audio: Preprocessed audio (fixed length)
        sr: Sample rate

    Returns:
        features: Concatenated feature vector (154-D)
            [0:40]    -> LPS (mean:20, std:20)
            [40:80]   -> LFCC (mean:20, std:20)
            [80:94]   -> Spectral Contrast (mean:7, std:7)
            [94:114]  -> MPE (4 orders x 5 scales = 20)
            [114:154] -> MFCC (mean:20, std:20)
    """
    # 1. LPS
    lps_feat = extract_lps(audio, sr=sr)

    # 2. LFCC
    S = np.abs(librosa.stft(audio))
    S_db = librosa.amplitude_to_db(S, ref=np.max)
    lfcc = librosa.feature.mfcc(S=S_db, n_mfcc=N_LFCC)
    lfcc_mean = lfcc.mean(axis=1)
    lfcc_std = lfcc.std(axis=1)
    lfcc_feat = np.concatenate([lfcc_mean, lfcc_std])

    # 3. Spectral Contrast
    contrast_feat = extract_spectral_contrast(audio, sr=sr)

    # 4. MPE
    mpe_feat = extract_mpe_features(audio, sr=sr, scales=MPE_SCALES, orders=MPE_ORDERS)

    # 5. MFCC (baseline)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)
    mfcc_mean = mfcc.mean(axis=1)
    mfcc_std = mfcc.std(axis=1)
    mfcc_feat = np.concatenate([mfcc_mean, mfcc_std])

    # Concatenate all
    features = np.concatenate([
        lps_feat,       # 40
        lfcc_feat,      # 40
        contrast_feat,  # 14
        mpe_feat,       # 20
        mfcc_feat       # 40
    ]).astype(np.float32)

    return features

# ============================================================
# EXTRACT FEATURES FROM ALL SAMPLES
# ============================================================

print("\nExtracting robust features...")

X_list = []

for audio in tqdm(preprocessed_audio, desc="Extracting", unit="audio"):
    features = extract_all_robust_features(audio, sr=SR)
    X_list.append(features)

X = np.array(X_list, dtype=np.float32)
y = labels

print(f"\nFeature extraction completed")
print(f"  Feature matrix shape: {X.shape}")
print(f"  Feature vector size: {X.shape[1]}")
print(f"  Label vector size: {y.shape[0]}")
print(f"  Memory: {X.nbytes / 1024**2:.2f} MB")

# ============================================================
# FEATURE STATISTICS
# ============================================================

print("\nFeature Statistics:")

# LPS
lps_features = X[:, :40]
print(f"  LPS (0-39):")
print(f"    Mean: {lps_features.mean():.3f}")
print(f"    Std: {lps_features.std():.3f}")
print(f"    Range: [{lps_features.min():.3f}, {lps_features.max():.3f}]")

# LFCC
lfcc_features = X[:, 40:80]
print(f"  LFCC (40-79):")
print(f"    Mean: {lfcc_features.mean():.3f}")
print(f"    Std: {lfcc_features.std():.3f}")
print(f"    Range: [{lfcc_features.min():.3f}, {lfcc_features.max():.3f}]")

# Spectral Contrast
contrast_features = X[:, 80:94]
print(f"  Spectral Contrast (80-93):")
print(f"    Mean: {contrast_features.mean():.3f}")
print(f"    Std: {contrast_features.std():.3f}")
print(f"    Range: [{contrast_features.min():.3f}, {contrast_features.max():.3f}]")

# MPE
mpe_features = X[:, 94:114]
print(f"  MPE (94-113):")
print(f"    Mean: {mpe_features.mean():.3f}")
print(f"    Std: {mpe_features.std():.3f}")
print(f"    Range: [{mpe_features.min():.3f}, {mpe_features.max():.3f}]")

# MFCC
mfcc_features = X[:, 114:]
print(f"  MFCC (114-153):")
print(f"    Mean: {mfcc_features.mean():.3f}")
print(f"    Std: {mfcc_features.std():.3f}")
print(f"    Range: [{mfcc_features.min():.3f}, {mfcc_features.max():.3f}]")

# ============================================================
# VISUALIZATION
# ============================================================

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

feature_groups = [
    ('LPS', lps_features, 'purple'),
    ('LFCC', lfcc_features, 'green'),
    ('Contrast', contrast_features, 'orange'),
    ('MPE', mpe_features, 'red'),
    ('MFCC', mfcc_features, 'blue')
]

for i, (name, features, color) in enumerate(feature_groups):
    axes[i].hist(features.flatten(), bins=50, alpha=0.7, color=color, edgecolor='black')
    axes[i].set_title(f'{name} Feature Distribution', fontsize=12, fontweight='bold')
    axes[i].set_xlabel('Feature Value')
    axes[i].set_ylabel('Frequency')
    axes[i].grid(True, alpha=0.3)

# Mean values comparison
feature_types = ['LPS', 'LFCC', 'Contrast', 'MPE', 'MFCC']
mean_values = [feat.mean() for _, feat, _ in feature_groups]
axes[5].bar(feature_types, mean_values,
           color=[c for _, _, c in feature_groups], alpha=0.7)
axes[5].set_title('Mean Feature Values by Type', fontsize=12, fontweight='bold')
axes[5].set_ylabel('Mean Value')
axes[5].grid(True, alpha=0.3, axis='y')
axes[5].tick_params(axis='x', rotation=45)

plt.suptitle('Robust Feature Extraction Analysis', fontsize=16, fontweight='bold')
plt.tight_layout()

fig_path = save_figure(fig, 'robust_feature_distributions.png')
print(f"\nFigure saved: {fig_path}")

# ============================================================
# SAVE FEATURES
# ============================================================

features_path = os.path.join(DIRS['artifacts'], 'features_robust.npz')
np.savez_compressed(
    features_path,
    X=X,
    y=y,
    feature_names=np.array(
        ['LPS'] * 40 +
        ['LFCC'] * 40 +
        ['Contrast'] * 14 +
        ['MPE'] * 20 +
        ['MFCC'] * 40
    )
)

print(f"Features saved: {features_path}")

# Clean memory
del audio_data, preprocessed_audio, X_list
gc.collect()

print("\n" + "="*80)
print("SECTION 3 COMPLETE")
print("="*80)

"""
SECTION 4: Feature Scaling and Train/Test Split
Prepare data for optimization experiments

"""

print("\n" + "="*80)
print("SECTION 4: Feature Scaling and Train/Test Split")
print("="*80)

# ============================================================
# TRAIN/TEST SPLIT
# ============================================================

TEST_SIZE = CONFIG['test_size']

print(f"\nTrain/Test Split ({int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)})...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=TEST_SIZE,
    random_state=SEED,
    stratify=y
)

print(f"Split completed:")
print(f"  Train: {len(y_train)} samples -> {Counter(y_train)}")
print(f"  Test:  {len(y_test)} samples -> {Counter(y_test)}")

# ============================================================
# FEATURE SCALING (STANDARDIZATION)
# ============================================================

print(f"\nApplying StandardScaler (z-score normalization)...")
print(f"  Formula: z = (x - mu) / sigma")
print(f"  Fitted on: Train set only (prevent data leakage)")

scaler = StandardScaler()

# Fit ONLY on training data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Scaling completed")

# ============================================================
# VERIFY SCALING
# ============================================================

print(f"\nScaling Verification:")
print(f"  Train (scaled):")
print(f"    Mean: {X_train_scaled.mean():.6f}")
print(f"    Std:  {X_train_scaled.std():.6f}")
print(f"  Test (scaled):")
print(f"    Mean: {X_test_scaled.mean():.6f}")
print(f"    Std:  {X_test_scaled.std():.6f}")

# ============================================================
# WHY SCALING MATTERS FOR OPTIMIZERS
# ============================================================

scaling_analysis = {
    'why_scaling_critical': {
        'gradient_magnitude': 'Features with large scales dominate gradient updates',
        'convergence_speed': 'Normalized features lead to faster convergence',
        'numerical_stability': 'Prevents numerical overflow/underflow',
        'loss_landscape': 'Creates more spherical loss surface for easier optimization',
        'learning_rate': 'Allows using consistent learning rates across features'
    },
    'method': 'StandardScaler (z-score normalization)',
    'formula': 'z = (x - mu) / sigma',
    'train_stats': {
        'mean': float(X_train_scaled.mean()),
        'std': float(X_train_scaled.std())
    },
    'test_stats': {
        'mean': float(X_test_scaled.mean()),
        'std': float(X_test_scaled.std())
    }
}

report_path = save_report(scaling_analysis, 'feature_scaling.json')
print(f"\nReport saved: {report_path}")

# ============================================================
# VISUALIZATION: BEFORE vs AFTER SCALING
# ============================================================

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Sample 5 features for visualization
sample_features = [0, 40, 80, 94, 114]
feature_names = ['LPS-0', 'LFCC-0', 'Contrast-0', 'MPE-0', 'MFCC-0']

# Before scaling - Train
axes[0, 0].boxplot([X_train[:, i] for i in sample_features],
                    labels=feature_names)
axes[0, 0].set_title('Train Features - Before Scaling', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('Feature Value')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(True, alpha=0.3, axis='y')

# After scaling - Train
axes[0, 1].boxplot([X_train_scaled[:, i] for i in sample_features],
                    labels=feature_names)
axes[0, 1].set_title('Train Features - After Scaling', fontsize=12, fontweight='bold')
axes[0, 1].set_ylabel('Standardized Value')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(True, alpha=0.3, axis='y')

# Before scaling - Test
axes[1, 0].boxplot([X_test[:, i] for i in sample_features],
                    labels=feature_names)
axes[1, 0].set_title('Test Features - Before Scaling', fontsize=12, fontweight='bold')
axes[1, 0].set_ylabel('Feature Value')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(True, alpha=0.3, axis='y')

# After scaling - Test
axes[1, 1].boxplot([X_test_scaled[:, i] for i in sample_features],
                    labels=feature_names)
axes[1, 1].set_title('Test Features - After Scaling', fontsize=12, fontweight='bold')
axes[1, 1].set_ylabel('Standardized Value')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.suptitle('Feature Scaling Analysis - Impact on Loss Landscape',
             fontsize=16, fontweight='bold')
plt.tight_layout()

fig_path = save_figure(fig, 'feature_scaling_comparison.png')
print(f"Figure saved: {fig_path}")

# ============================================================
# SAVE SCALER
# ============================================================

scaler_path = save_model(scaler, 'scaler.pkl')
print(f"Scaler saved: {scaler_path}")

# ============================================================
# PREPARE LFCC+CONTRAST+MPE FOR SE-RESNET18
# ============================================================

print(f"\nPreparing separate scaler for SE-ResNet18 (LFCC+Contrast+MPE only)...")

# Extract LFCC + Contrast + MPE features (indices 40-113)
# LFCC: 40-79 (40 features)
# Contrast: 80-93 (14 features)
# MPE: 94-113 (20 features)
# Total: 74 features

X_train_lfcc_contrast_mpe = X_train[:, 40:114]
X_test_lfcc_contrast_mpe = X_test[:, 40:114]

# Separate scaler
scaler_lfcc_contrast_mpe = StandardScaler()
X_train_lfcc_contrast_mpe_scaled = scaler_lfcc_contrast_mpe.fit_transform(X_train_lfcc_contrast_mpe)
X_test_lfcc_contrast_mpe_scaled = scaler_lfcc_contrast_mpe.transform(X_test_lfcc_contrast_mpe)

print(f"LFCC+Contrast+MPE scaler created:")
print(f"  Features: 74 (LFCC:40 + Contrast:14 + MPE:20)")
print(f"  For: SE-ResNet18 model")

# Save scaler
scaler_lfcc_contrast_mpe_path = save_model(scaler_lfcc_contrast_mpe, 'scaler_lfcc_contrast_mpe.pkl')
print(f"Scaler saved: {scaler_lfcc_contrast_mpe_path}")

print("\n" + "="*80)
print("SECTION 4 COMPLETE")
print("="*80)

"""
SECTION 5: Optimizer Comparison Study
CORE OF OPTIMIZATION THEORY PROJECT

Compare gradient-based optimizers on identical architecture:
- SGD (Stochastic Gradient Descent with Momentum)
- Adam (Adaptive Moment Estimation)
- AdamW (Adam with Decoupled Weight Decay)
- RMSProp (Root Mean Square Propagation)

Analysis Focus:
- Convergence speed
- Training stability
- Generalization gap
- Gradient norm dynamics
- Loss landscape navigation
"""

print("\n" + "="*80)
print("SECTION 5: Optimizer Comparison Study")
print("="*80)

# ============================================================
# MODEL DEFINITION: IMPROVED CNN
# ============================================================

class ImprovedCNN(nn.Module):
    """
    1D-CNN for Deepfake Detection
    Input: 154 features (all robust features)

    Architecture:
    - 3 Conv blocks with BatchNorm and Dropout
    - Adaptive pooling
    - 2-layer classifier
    """
    def __init__(self, input_dim=154):
        super().__init__()

        self.conv_layers = nn.Sequential(
            # Block 1
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(0.3),

            # Block 2
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(0.3),

            # Block 3
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )

        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        x = x.unsqueeze(1)  # (batch, 154) -> (batch, 1, 154)
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# ============================================================
# LOSS FUNCTION: FOCAL LOSS
# ============================================================

class FocalLoss(nn.Module):
    """
    Focal Loss for hard example mining
    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)

    Benefits:
    - Down-weights easy examples
    - Focuses on hard examples
    - Better for imbalanced datasets
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, logits, targets):
        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        probs = torch.sigmoid(logits)
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal_weight = (1 - pt) ** self.gamma
        focal_loss = self.alpha * focal_weight * bce_loss
        return focal_loss.mean()

criterion = FocalLoss(alpha=CONFIG['focal_alpha'], gamma=CONFIG['focal_gamma'])

# ============================================================
# PREPARE DATA TENSORS
# ============================================================

X_train_t = torch.FloatTensor(X_train_scaled).to(device)
y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)
X_test_t = torch.FloatTensor(X_test_scaled).to(device)
y_test_t = torch.FloatTensor(y_test).unsqueeze(1).to(device)

print(f"\nData tensors prepared:")
print(f"  Train: {X_train_t.shape}")
print(f"  Test: {X_test_t.shape}")

# ============================================================
# OPTIMIZER TRAINING FUNCTION
# ============================================================

def train_with_optimizer(optimizer_name, optimizer_config, epochs=CONFIG['epochs_cnn']):
    """
    Train CNN with specific optimizer and track metrics

    Args:
        optimizer_name: Name of optimizer (sgd, adam, adamw, rmsprop)
        optimizer_config: Optimizer hyperparameters
        epochs: Number of training epochs

    Returns:
        dict: Training metrics and model state
    """
    print(f"\n{'='*80}")
    print(f"Training with {optimizer_name.upper()}")
    print(f"  Config: {optimizer_config}")
    print(f"{'='*80}")

    # Initialize model
    set_seed(SEED)  # Ensure same initialization
    model = ImprovedCNN(input_dim=154).to(device)

    # Initialize optimizer
    if optimizer_name == 'sgd':
        optimizer = optim.SGD(
            model.parameters(),
            lr=optimizer_config['lr'],
            momentum=optimizer_config['momentum'],
            weight_decay=optimizer_config['weight_decay'],
            nesterov=optimizer_config['nesterov']
        )
    elif optimizer_name == 'adam':
        optimizer = optim.Adam(
            model.parameters(),
            lr=optimizer_config['lr'],
            betas=optimizer_config['betas'],
            eps=optimizer_config['eps'],
            weight_decay=optimizer_config['weight_decay']
        )
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(
            model.parameters(),
            lr=optimizer_config['lr'],
            betas=optimizer_config['betas'],
            eps=optimizer_config['eps'],
            weight_decay=optimizer_config['weight_decay']
        )
    elif optimizer_name == 'rmsprop':
        optimizer = optim.RMSprop(
            model.parameters(),
            lr=optimizer_config['lr'],
            alpha=optimizer_config['alpha'],
            eps=optimizer_config['eps'],
            weight_decay=optimizer_config['weight_decay']
        )

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # Tracking metrics
    metrics = {
        'train_losses': [],
        'train_accs': [],
        'test_accs': [],
        'gradient_norms': [],
        'learning_rates': []
    }

    # Training loop
    for epoch in range(epochs):
        # TRAIN
        model.train()
        optimizer.zero_grad()

        # Forward pass
        outputs = model(X_train_t)
        loss = criterion(outputs, y_train_t)

        # Backward pass
        loss.backward()

        # Track gradient norm (IMPORTANT FOR OPTIMIZER ANALYSIS)
        grad_norm = track_gradient_norm(model)
        metrics['gradient_norms'].append(grad_norm)

        # Optimizer step
        optimizer.step()
        scheduler.step()

        # Track learning rate
        metrics['learning_rates'].append(optimizer.param_groups[0]['lr'])

        # Train metrics
        metrics['train_losses'].append(loss.item())

        with torch.no_grad():
            train_preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy().astype(int).flatten()
            train_acc = accuracy_score(y_train, train_preds)
            metrics['train_accs'].append(train_acc)

        # TEST (every 5 epochs)
        if (epoch + 1) % 5 == 0:
            model.eval()
            with torch.no_grad():
                test_outputs = model(X_test_t)
                test_preds = (torch.sigmoid(test_outputs) > 0.5).cpu().numpy().astype(int).flatten()
                test_acc = accuracy_score(y_test, test_preds)
                metrics['test_accs'].append(test_acc)

            print(f"  Epoch [{epoch+1:2d}/{epochs}] "
                  f"Loss: {loss.item():.4f} | "
                  f"Train: {train_acc:.4f} | "
                  f"Test: {test_acc:.4f} | "
                  f"GradNorm: {grad_norm:.4f}")

    # Final evaluation
    model.eval()
    with torch.no_grad():
        y_logits = model(X_test_t).cpu().numpy().flatten()
        y_proba = torch.sigmoid(torch.FloatTensor(y_logits)).numpy()
        y_pred = (y_proba > 0.5).astype(int)

    final_acc = accuracy_score(y_test, y_pred)
    final_f1 = f1_score(y_test, y_pred)
    final_auc = roc_auc_score(y_test, y_proba)

    print(f"\n  Final Results:")
    print(f"    Accuracy: {final_acc:.4f}")
    print(f"    F1-Score: {final_f1:.4f}")
    print(f"    ROC-AUC: {final_auc:.4f}")

    return {
        'model': model,
        'metrics': metrics,
        'final_scores': {
            'accuracy': final_acc,
            'f1_score': final_f1,
            'roc_auc': final_auc
        },
        'optimizer_name': optimizer_name,
        'optimizer_config': optimizer_config
    }

# ============================================================
# RUN OPTIMIZER COMPARISON
# ============================================================

print("\nStarting Optimizer Comparison Experiments...")
print("This is the core analysis for Optimization Theory course")

optimizer_results = {}

for opt_name, opt_config in CONFIG['optimizers'].items():
    result = train_with_optimizer(opt_name, opt_config)
    optimizer_results[opt_name] = result

    # Save model
    model_path = save_model(result['model'], f'cnn_{opt_name}_model.pth')
    print(f"  Model saved: {model_path}")

# ============================================================
# COMPARATIVE ANALYSIS
# ============================================================

print("\n" + "="*80)
print("OPTIMIZER COMPARISON ANALYSIS")
print("="*80)

# Create comparison table
comparison_data = []
for opt_name, result in optimizer_results.items():
    comparison_data.append({
        'Optimizer': opt_name.upper(),
        'Final Accuracy': result['final_scores']['accuracy'],
        'Final F1-Score': result['final_scores']['f1_score'],
        'Final AUC': result['final_scores']['roc_auc'],
        'Final Loss': result['metrics']['train_losses'][-1],
        'Avg Gradient Norm': np.mean(result['metrics']['gradient_norms']),
        'Convergence Speed': result['metrics']['train_accs'].index(
            max(result['metrics']['train_accs'])
        ) + 1 if max(result['metrics']['train_accs']) > 0.9 else CONFIG['epochs_cnn']
    })

df_comparison = pd.DataFrame(comparison_data)
print("\n" + df_comparison.to_string(index=False))

# Find best optimizer
best_idx = df_comparison['Final Accuracy'].idxmax()
best_optimizer = df_comparison.iloc[best_idx]['Optimizer']

print(f"\nBest Optimizer: {best_optimizer}")
print(f"  Accuracy: {df_comparison.iloc[best_idx]['Final Accuracy']:.4f}")
print(f"  F1-Score: {df_comparison.iloc[best_idx]['Final F1-Score']:.4f}")
print(f"  AUC: {df_comparison.iloc[best_idx]['Final AUC']:.4f}")

# Save comparison report
comparison_report = {
    'experiment': 'Optimizer Comparison Study',
    'model': 'ImprovedCNN (154-D input)',
    'dataset': 'ASVspoof 2019 LA',
    'optimizers_tested': list(CONFIG['optimizers'].keys()),
    'results': comparison_data,
    'best_optimizer': best_optimizer.lower(),
    'analysis': {
        'convergence': 'Comparison of convergence speed across optimizers',
        'stability': 'Gradient norm tracking shows optimizer stability',
        'generalization': 'Train-test gap indicates generalization ability'
    }
}

report_path = save_report(comparison_report, 'optimizer_comparison_report.json')
print(f"\nComparison report saved: {report_path}")

print("\n" + "="*80)
print("SECTION 5 COMPLETE")
print("="*80)

"""
SECTION 6: Optimizer Analysis Visualization
Visualize optimizer behavior and dynamics

Key Visualizations:
1. Training Loss Curves
2. Training Accuracy Curves
3. Test Accuracy Evolution
4. Gradient Norm Dynamics
5. Learning Rate Schedules
6. Final Metrics Comparison
"""

print("\n" + "="*80)
print("SECTION 6: Optimizer Analysis Visualization")
print("="*80)

# ============================================================
# COMPREHENSIVE OPTIMIZER VISUALIZATION
# ============================================================

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# Color scheme for optimizers
colors = {
    'sgd': '#e74c3c',
    'adam': '#3498db',
    'adamw': '#2ecc71',
    'rmsprop': '#f39c12'
}

# 1. Training Loss
ax1 = fig.add_subplot(gs[0, :2])
for opt_name, result in optimizer_results.items():
    ax1.plot(result['metrics']['train_losses'],
            label=opt_name.upper(),
            color=colors[opt_name],
            linewidth=2, alpha=0.8)
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Training Loss')
ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

# 2. Training Accuracy
ax2 = fig.add_subplot(gs[0, 2])
for opt_name, result in optimizer_results.items():
    ax2.plot(result['metrics']['train_accs'],
            label=opt_name.upper(),
            color=colors[opt_name],
            linewidth=2, alpha=0.8)
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Training Accuracy')
ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_ylim([0.5, 1.05])

# 3. Test Accuracy Evolution
ax3 = fig.add_subplot(gs[1, :2])
epochs_tested = list(range(5, CONFIG['epochs_cnn']+1, 5))
for opt_name, result in optimizer_results.items():
    ax3.plot(epochs_tested, result['metrics']['test_accs'],
            'o-', label=opt_name.upper(),
            color=colors[opt_name],
            linewidth=2, markersize=8, alpha=0.8)
ax3.set_xlabel('Epoch')
ax3.set_ylabel('Test Accuracy')
ax3.set_title('Test Accuracy Evolution', fontsize=14, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)
ax3.set_ylim([0.5, 1.05])

# 4. Gradient Norm Dynamics
ax4 = fig.add_subplot(gs[1, 2])
for opt_name, result in optimizer_results.items():
    ax4.plot(result['metrics']['gradient_norms'],
            label=opt_name.upper(),
            color=colors[opt_name],
            linewidth=2, alpha=0.8)
ax4.set_xlabel('Epoch')
ax4.set_ylabel('Gradient Norm')
ax4.set_title('Gradient Norm Dynamics', fontsize=14, fontweight='bold')
ax4.legend(fontsize=9)
ax4.grid(True, alpha=0.3)
ax4.set_yscale('log')

# 5. Learning Rate Schedule
ax5 = fig.add_subplot(gs[2, 0])
for opt_name, result in optimizer_results.items():
    ax5.plot(result['metrics']['learning_rates'],
            label=opt_name.upper(),
            color=colors[opt_name],
            linewidth=2, alpha=0.8)
ax5.set_xlabel('Epoch')
ax5.set_ylabel('Learning Rate')
ax5.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
ax5.legend(fontsize=9)
ax5.grid(True, alpha=0.3)
ax5.set_yscale('log')

# 6. Final Metrics Comparison
ax6 = fig.add_subplot(gs[2, 1])
metrics = ['Accuracy', 'F1-Score', 'AUC']
x = np.arange(len(metrics))
width = 0.2

for i, opt_name in enumerate(optimizer_results.keys()):
    result = optimizer_results[opt_name]
    scores = [
        result['final_scores']['accuracy'],
        result['final_scores']['f1_score'],
        result['final_scores']['roc_auc']
    ]
    ax6.bar(x + i*width, scores, width,
           label=opt_name.upper(),
           color=colors[opt_name],
           alpha=0.7)

ax6.set_ylabel('Score')
ax6.set_title('Final Metrics Comparison', fontsize=14, fontweight='bold')
ax6.set_xticks(x + width * 1.5)
ax6.set_xticklabels(metrics)
ax6.legend(fontsize=9)
ax6.grid(True, alpha=0.3, axis='y')
ax6.set_ylim([0.8, 1.05])

# 7. Convergence Speed
ax7 = fig.add_subplot(gs[2, 2])
convergence_speeds = []
for opt_name, result in optimizer_results.items():
    train_accs = result['metrics']['train_accs']
    # Find epoch where accuracy > 0.9
    conv_speed = next((i for i, acc in enumerate(train_accs) if acc > 0.9), len(train_accs))
    convergence_speeds.append(conv_speed)

ax7.barh(list(optimizer_results.keys()), convergence_speeds,
        color=[colors[opt] for opt in optimizer_results.keys()],
        alpha=0.7)
ax7.set_xlabel('Epochs to 90% Accuracy')
ax7.set_title('Convergence Speed', fontsize=14, fontweight='bold')
ax7.grid(True, alpha=0.3, axis='x')
ax7.invert_yaxis()

plt.suptitle('Optimizer Comparison Analysis - Optimization Theory Project',
             fontsize=18, fontweight='bold')

fig_path = save_figure(fig, 'optimizer_comparison_analysis.png')
print(f"\nFigure saved: {fig_path}")

# ============================================================
# OPTIMIZER BEHAVIOR ANALYSIS
# ============================================================

print("\nOptimizer Behavior Analysis:")

for opt_name, result in optimizer_results.items():
    print(f"\n{opt_name.upper()}:")

    # Convergence analysis
    train_accs = result['metrics']['train_accs']
    epochs_to_90 = next((i for i, acc in enumerate(train_accs) if acc > 0.9), len(train_accs))
    print(f"  Convergence: {epochs_to_90} epochs to 90% accuracy")

    # Stability analysis
    grad_norms = result['metrics']['gradient_norms']
    grad_std = np.std(grad_norms)
    print(f"  Stability: Gradient norm std = {grad_std:.4f}")

    # Generalization analysis
    final_train_acc = train_accs[-1]
    final_test_acc = result['final_scores']['accuracy']
    gen_gap = final_train_acc - final_test_acc
    print(f"  Generalization gap: {gen_gap:.4f}")

    # Loss reduction
    initial_loss = result['metrics']['train_losses'][0]
    final_loss = result['metrics']['train_losses'][-1]
    loss_reduction = (initial_loss - final_loss) / initial_loss * 100
    print(f"  Loss reduction: {loss_reduction:.1f}%")

print("\n" + "="*80)
print("SECTION 6 COMPLETE")
print("="*80)

"""
SECTION 7: Gradient-Based Explainability
Apply gradient analysis methods to best CNN model

Methods:
1. Saliency Maps: |âˆ‚f(x)/âˆ‚x|
2. Integrated Gradients: (x - baseline) Ã— âˆ« gradients
3. FGSM Attack: x_adv = x + ÎµÂ·sign(âˆ‡L)

Focus: Understanding model decisions through gradient flow
"""

print("\n" + "="*80)
print("SECTION 7: Gradient-Based Explainability")
print("="*80)

# ============================================================
# SELECT BEST MODEL FOR ANALYSIS
# ============================================================

# Find best optimizer result
best_opt = max(optimizer_results.items(),
              key=lambda x: x[1]['final_scores']['accuracy'])
best_opt_name = best_opt[0]
best_model = best_opt[1]['model']

print(f"\nAnalyzing model trained with: {best_opt_name.upper()}")
print(f"  Test Accuracy: {best_opt[1]['final_scores']['accuracy']:.4f}")
print(f"  Applied to: CNN only (direct feature-decision mapping)")

# ============================================================
# SALIENCY MAPS
# ============================================================

print("\n[1] Computing Saliency Maps...")

def compute_saliency_map(model, x, target_class=1):
    """
    Gradient saliency map
    |âˆ‚f(x)/âˆ‚x| - Shows which features model focuses on

    Args:
        model: Trained CNN
        x: Input features (154-D)
        target_class: 1=spoof, 0=bonafide

    Returns:
        saliency: Gradient magnitudes per feature
    """
    model.eval()
    x = x.clone().detach().requires_grad_(True)

    output = model(x)

    if target_class == 1:
        loss = output.sum()
    else:
        loss = -output.sum()

    loss.backward()

    saliency = x.grad.abs().cpu().numpy()

    return saliency

# Compute for spoof samples
spoof_indices = np.where(y_test == 1)[0][:10]

saliency_spoof = []

for idx in tqdm(spoof_indices, desc="Computing saliency", unit="sample"):
    x = X_test_t[idx:idx+1]
    sal = compute_saliency_map(best_model, x, target_class=1)
    saliency_spoof.append(sal.flatten())

avg_saliency_spoof = np.mean(saliency_spoof, axis=0)

print(f"Saliency maps computed")

# Top 10 salient features
top_10_sal_idx = np.argsort(avg_saliency_spoof)[-10:][::-1]

print(f"\nTop 10 Salient Features (Spoof):")
for rank, idx in enumerate(top_10_sal_idx, 1):
    if idx < 20:
        feat_type = 'LPS (mean)'
    elif idx < 40:
        feat_type = 'LPS (std)'
    elif idx < 60:
        feat_type = 'LFCC (mean)'
    elif idx < 80:
        feat_type = 'LFCC (std)'
    elif idx < 87:
        feat_type = 'Contrast (mean)'
    elif idx < 94:
        feat_type = 'Contrast (std)'
    elif idx < 114:
        feat_type = 'MPE'
    elif idx < 134:
        feat_type = 'MFCC (mean)'
    else:
        feat_type = 'MFCC (std)'

    print(f"  {rank:2d}. Feature {idx:3d} ({feat_type}) -> Saliency: {avg_saliency_spoof[idx]:.4f}")

# ============================================================
# INTEGRATED GRADIENTS
# ============================================================

print("\n[2] Computing Integrated Gradients...")

def integrated_gradients(model, x, target_class=1, steps=50):
    """
    Integrated Gradients
    IG = (x - baseline) Ã— âˆ« gradients

    More stable and reliable than simple gradients

    Args:
        model: Trained CNN
        x: Input features
        target_class: 1=spoof, 0=bonafide
        steps: Integration steps

    Returns:
        attributions: Feature attributions
    """
    model.eval()

    baseline = torch.zeros_like(x)
    alphas = torch.linspace(0, 1, steps).to(device)

    gradients = []

    for alpha in alphas:
        x_inter = baseline + alpha * (x - baseline)
        x_inter = x_inter.clone().detach().requires_grad_(True)

        output = model(x_inter)

        if target_class == 1:
            loss = output.sum()
        else:
            loss = -output.sum()

        loss.backward()
        gradients.append(x_inter.grad.cpu().numpy())

    avg_grads = np.mean(gradients, axis=0)
    attributions = (x - baseline).cpu().numpy() * avg_grads

    return attributions.flatten()

# Compute for 5 spoof samples
ig_spoof = []

for idx in tqdm(spoof_indices[:5], desc="Computing IG", unit="sample"):
    x = X_test_t[idx:idx+1]
    ig = integrated_gradients(best_model, x, target_class=1, steps=50)
    ig_spoof.append(ig)

avg_ig_spoof = np.mean(ig_spoof, axis=0)

print(f"Integrated gradients computed")

# ============================================================
# FGSM ADVERSARIAL ATTACK
# ============================================================

print("\n[3] Running FGSM Attack...")

def fgsm_attack(model, x, y, epsilon):
    """
    Fast Gradient Sign Method
    x_adv = x + ÎµÂ·sign(âˆ‡L)

    Tests model robustness to adversarial perturbations

    Args:
        model: Trained CNN
        x: Input features
        y: True labels
        epsilon: Perturbation magnitude

    Returns:
        x_adv: Adversarial examples
        success_rate: Attack success rate
    """
    model.eval()

    x = x.clone().detach().requires_grad_(True)

    output = model(x)
    loss = F.binary_cross_entropy_with_logits(output, y)

    loss.backward()

    x_adv = x + epsilon * x.grad.sign()
    x_adv = torch.clamp(x_adv, -3, 3)

    model.eval()
    with torch.no_grad():
        original_pred = (torch.sigmoid(model(x)) > 0.5).cpu().numpy()
        adv_pred = (torch.sigmoid(model(x_adv)) > 0.5).cpu().numpy()
        success_rate = (original_pred != adv_pred).mean()

    return x_adv, success_rate

epsilons = [0.0, 0.05, 0.1, 0.15, 0.2]
fgsm_results = []

print(f"Testing FGSM with epsilons: {epsilons}")

for eps in epsilons:
    x_adv, success_rate = fgsm_attack(best_model, X_test_t, y_test_t, eps)

    best_model.eval()
    with torch.no_grad():
        y_pred_adv = (torch.sigmoid(best_model(x_adv)) > 0.5).cpu().numpy().astype(int).flatten()
        acc_adv = accuracy_score(y_test, y_pred_adv)

    fgsm_results.append({
        'epsilon': eps,
        'accuracy': acc_adv,
        'attack_success': success_rate
    })

    print(f"  Îµ={eps:.2f} -> Acc: {acc_adv:.3f} | Attack Success: {success_rate:.3f}")

df_fgsm = pd.DataFrame(fgsm_results)

print(f"FGSM attack analysis completed")

# ============================================================
# COMPREHENSIVE VISUALIZATION
# ============================================================

fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# Feature colors: LPS(40) + LFCC(40) + Contrast(14) + MPE(20) + MFCC(40) = 154
colors_feat = (
    ['purple']*40 +      # LPS
    ['green']*40 +       # LFCC
    ['orange']*14 +      # Contrast
    ['red']*20 +         # MPE
    ['blue']*40          # MFCC
)

# 1. Saliency Map
ax1 = fig.add_subplot(gs[0, :2])
ax1.bar(range(154), avg_saliency_spoof, color=colors_feat, alpha=0.7, edgecolor='black', linewidth=0.5)
ax1.axvline(39.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax1.axvline(79.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax1.axvline(93.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax1.axvline(113.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax1.set_title('Gradient Saliency Map (Spoof Samples)', fontsize=14, fontweight='bold')
ax1.set_xlabel('Feature Index (LPS | LFCC | Contrast | MPE | MFCC)')
ax1.set_ylabel('Saliency')
ax1.grid(True, alpha=0.3, axis='y')

# 2. Top 10 salient features
ax2 = fig.add_subplot(gs[0, 2])
top_10_vals = avg_saliency_spoof[top_10_sal_idx]
ax2.barh(range(10), top_10_vals, color='red', alpha=0.7)
ax2.set_yticks(range(10))
ax2.set_yticklabels([f'F{idx}' for idx in top_10_sal_idx])
ax2.invert_yaxis()
ax2.set_title('Top 10 Salient Features', fontsize=12, fontweight='bold')
ax2.set_xlabel('Saliency')
ax2.grid(True, alpha=0.3, axis='x')

# 3. Integrated Gradients
ax3 = fig.add_subplot(gs[1, :2])
ax3.bar(range(154), np.abs(avg_ig_spoof), color=colors_feat, alpha=0.7, edgecolor='black', linewidth=0.5)
ax3.axvline(39.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax3.axvline(79.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax3.axvline(93.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax3.axvline(113.5, color='black', linestyle='--', linewidth=1, alpha=0.5)
ax3.set_title('Integrated Gradients (Spoof Samples)', fontsize=14, fontweight='bold')
ax3.set_xlabel('Feature Index (LPS | LFCC | Contrast | MPE | MFCC)')
ax3.set_ylabel('Attribution')
ax3.grid(True, alpha=0.3, axis='y')

# 4. FGSM Attack Results
ax4 = fig.add_subplot(gs[1, 2])
ax4.plot(df_fgsm['epsilon'], df_fgsm['accuracy'], 'o-', linewidth=2, markersize=8, color='red')
ax4.set_title('FGSM Attack Impact', fontsize=12, fontweight='bold')
ax4.set_xlabel('Epsilon (Îµ)')
ax4.set_ylabel('Accuracy')
ax4.grid(True, alpha=0.3)
ax4.set_ylim([0, 1.05])

# 5. Feature Group Saliency
ax5 = fig.add_subplot(gs[2, 0])
feature_groups = {
    'LPS': avg_saliency_spoof[:40].mean(),
    'LFCC': avg_saliency_spoof[40:80].mean(),
    'Contrast': avg_saliency_spoof[80:94].mean(),
    'MPE': avg_saliency_spoof[94:114].mean(),
    'MFCC': avg_saliency_spoof[114:].mean()
}
ax5.bar(feature_groups.keys(), feature_groups.values(),
       color=['purple', 'green', 'orange', 'red', 'blue'],
       alpha=0.7)
ax5.set_title('Average Saliency by Feature Group', fontsize=12, fontweight='bold')
ax5.set_ylabel('Mean Saliency')
ax5.grid(True, alpha=0.3, axis='y')
ax5.tick_params(axis='x', rotation=45)

# 6. Feature Group IG Attribution
ax6 = fig.add_subplot(gs[2, 1])
ig_groups = {
    'LPS': np.abs(avg_ig_spoof[:40]).mean(),
    'LFCC': np.abs(avg_ig_spoof[40:80]).mean(),
    'Contrast': np.abs(avg_ig_spoof[80:94]).mean(),
    'MPE': np.abs(avg_ig_spoof[94:114]).mean(),
    'MFCC': np.abs(avg_ig_spoof[114:]).mean()
}
ax6.bar(ig_groups.keys(), ig_groups.values(),
       color=['purple', 'green', 'orange', 'red', 'blue'],
       alpha=0.7)
ax6.set_title('Average IG Attribution by Feature Group', fontsize=12, fontweight='bold')
ax6.set_ylabel('Mean Attribution')
ax6.grid(True, alpha=0.3, axis='y')
ax6.tick_params(axis='x', rotation=45)

# 7. Robustness to Perturbation
ax7 = fig.add_subplot(gs[2, 2])
ax7.bar(df_fgsm['epsilon'].astype(str), df_fgsm['attack_success'],
       color='orange', alpha=0.7)
ax7.set_title('FGSM Attack Success Rate', fontsize=12, fontweight='bold')
ax7.set_xlabel('Epsilon')
ax7.set_ylabel('Attack Success Rate')
ax7.grid(True, alpha=0.3, axis='y')

plt.suptitle(f'Gradient-Based Explainability Analysis ({best_opt_name.upper()} Optimizer)',
             fontsize=18, fontweight='bold')

fig_path = save_figure(fig, 'gradient_explainability_analysis.png')
print(f"\nFigure saved: {fig_path}")

# ============================================================
# SAVE GRADIENT ANALYSIS REPORT
# ============================================================

gradient_report = {
    'model': 'CNN',
    'optimizer': best_opt_name,
    'methods': ['Saliency Maps', 'Integrated Gradients', 'FGSM Attack'],
    'saliency': {
        'top_10_features': [int(x) for x in top_10_sal_idx.tolist()],
        'mean_saliency': float(avg_saliency_spoof.mean()),
        'max_saliency': float(avg_saliency_spoof.max()),
        'feature_group_means': {k: float(v) for k, v in feature_groups.items()}
    },
    'integrated_gradients': {
        'mean_attribution': float(np.abs(avg_ig_spoof).mean()),
        'max_attribution': float(np.abs(avg_ig_spoof).max()),
        'feature_group_means': {k: float(v) for k, v in ig_groups.items()}
    },
    'fgsm_attack': df_fgsm.to_dict('records'),
    'interpretation': {
        'saliency': 'Shows which features CNN focuses on for decisions',
        'ig': 'Stable attribution method integrating gradients along path',
        'fgsm': 'Reveals model vulnerability to adversarial perturbations'
    }
}

report_path = save_report(gradient_report, 'gradient_analysis_report.json')
print(f"Report saved: {report_path}")

print("\n" + "="*80)
print("SECTION 7 COMPLETE")
print("="*80)

"""
SECTION 8: THRESHOLD OPTIMIZATION
"""

import torch
import torch.nn as nn
import numpy as np
import os
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_curve, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive

# ---------------------------------------------------------
# 1. AYARLAR VE PATH TANIMLAMALARI
# ---------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
drive.mount('/content/drive', force_remount=False)
BASE_DIR = "/content/drive/MyDrive/DeepfakeOptimization_Clean"
SEED = 42  # EÄŸitimdeki seed ile aynÄ± olmalÄ±!

# ---------------------------------------------------------
# 2. MODEL SINIFINI TEKRAR TANIMLA (YÃ¼klemek iÃ§in ÅŸart)
# ---------------------------------------------------------
class ImprovedCNN(nn.Module):
    def __init__(self, input_dim=154):
        super().__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(0.3),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(0.3),
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# ---------------------------------------------------------
# 3. VERÄ°YÄ° VE MODELÄ° YÃœKLE
# ---------------------------------------------------------
print("Veriler ve Model YÃ¼kleniyor...")

# A. Veriyi YÃ¼kle (features_robust.npz)
features_path = os.path.join(BASE_DIR, 'artifacts', 'features_robust.npz')
data = np.load(features_path)
X = data['X']
y = data['y']

# B. Test Setini AyÄ±r (EÄŸitimdekiyle birebir aynÄ± olmasÄ± iÃ§in Seed 42 kullanÄ±yoruz)
_, X_test, _, y_test = train_test_split(
    X, y, test_size=0.2, random_state=SEED, stratify=y
)

# C. Scaler'Ä± YÃ¼kle ve Test Verisini Normalize Et
scaler_path = os.path.join(BASE_DIR, 'models', 'scaler.pkl')
scaler = joblib.load(scaler_path)
X_test_scaled = scaler.transform(X_test)

# Tensor'a Ã‡evir
X_test_t = torch.FloatTensor(X_test_scaled).to(device)

# D. En Ä°yi Modeli (AdamW) YÃ¼kle
model_path = os.path.join(BASE_DIR, 'models', 'cnn_adamw_model.pth')
model = ImprovedCNN(input_dim=154).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()

print("YÃ¼kleme TamamlandÄ±.")

# ---------------------------------------------------------
# 4. THRESHOLD OPTIMIZATION (EÅŸik DeÄŸeri AyarÄ±)
# ---------------------------------------------------------
print("\n En Ä°yi EÅŸik DeÄŸeri HesaplanÄ±yor...")

# Tahminleri Al (Logits -> Probability)
with torch.no_grad():
    logits = model(X_test_t)
    y_probs = torch.sigmoid(logits).cpu().numpy().flatten()

# ROC EÄŸrisini Ã‡Ä±kar
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

# Youden's J Statistic (J = TPR - FPR) ile En Ä°yi EÅŸiÄŸi Bul
J = tpr - fpr
best_idx = np.argmax(J)
best_threshold = thresholds[best_idx]

print(f"\n" + "="*50)
print(f" OPTÄ°MÄ°ZASYON SONUCU")
print(f"="*50)
print(f"Eski Threshold: 0.5000")
print(f"Yeni (Optimal) Threshold: {best_threshold:.4f}")

# ---------------------------------------------------------
# 5. YENÄ° SKORLARI HESAPLA
# ---------------------------------------------------------
# Eski Tahminler (0.5 ile)
old_preds = (y_probs > 0.5).astype(int)
old_acc = accuracy_score(y_test, old_preds)

# Yeni Tahminler (Optimal Threshold ile)
new_preds = (y_probs > best_threshold).astype(int)
new_acc = accuracy_score(y_test, new_preds)

print(f"\n PERFORMANS KARÅžILAÅžTIRMASI:")
print(f"  Eski Accuracy: {old_acc:.4f} (%{old_acc*100:.2f})")
print(f"  Yeni Accuracy: {new_acc:.4f} (%{new_acc*100:.2f})")
print(f"   fark: +{(new_acc - old_acc)*100:.2f} puan artÄ±ÅŸ!")

print("\n Yeni SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_test, new_preds, target_names=['Bonafide', 'Spoof']))

# ---------------------------------------------------------
# 6. GÃ–RSELLEÅžTÄ°RME (ROC Curve & Confusion Matrix)
# ---------------------------------------------------------
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ROC Curve
axes[0].plot(fpr, tpr, label=f'AdamW (AUC = {0.9956:.4f})', color='green', linewidth=2)
axes[0].scatter(fpr[best_idx], tpr[best_idx], marker='o', color='black', label=f'Best Threshold ({best_threshold:.2f})', zorder=5)
axes[0].plot([0, 1], [0, 1], 'r--')
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('ROC Curve & Optimal Threshold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Confusion Matrix
cm = confusion_matrix(y_test, new_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],
            xticklabels=['Bonafide', 'Spoof'], yticklabels=['Bonafide', 'Spoof'])
axes[1].set_title(f'Confusion Matrix (Threshold: {best_threshold:.2f})')
axes[1].set_ylabel('GerÃ§ek Etiket')
axes[1].set_xlabel('Tahmin')

plt.tight_layout()
plt.show()

"""
SECTION 9: Cross-Dataset Evaluation (InTheWild)
Test trained optimizers on unseen real-world data

Challenge: Domain Shift
- Source Domain: ASVspoof 2019 LA (Controlled, Lab)
- Target Domain: InTheWild (Uncontrolled, Real-world noise, Codecs)
"""

print("\n" + "="*80)
print("SECTION 9: Cross-Dataset Evaluation (InTheWild)")
print("="*80)

# ============================================================
# 1. DATA INGESTION (INTHEWILD SUBSET)
# ============================================================

N_ITW_SAMPLES = 200

print(f"\nInTheWild Verisi YÃ¼kleniyor (Hedef: {N_ITW_SAMPLES} Ã¶rnek)...")

itw_audio = [] # Initialize itw_audio as a list to store dicts
itw_labels = [] # Initialize itw_labels
cnt_r, cnt_f = 0, 0
limit = N_ITW_SAMPLES // 2

try: # Moved try block to correctly wrap the data loading and processing
    ds_itw = load_dataset("UncovAI/InTheWild", split="train",
                          streaming=True).shuffle(seed=CONFIG['seed'], buffer_size=100)

    # Correct total for tqdm, and ensure itw_audio contains dictionaries
    for ex in tqdm(ds_itw, total=N_ITW_SAMPLES, desc="ITW"): # Use N_ITW_SAMPLES
        path = str(ex["audio"]["path"])
        lbl = -1 # Initialize lbl

        if "REAL" in path or "Real" in path:
            if cnt_r >= limit:
                continue
            lbl = 0 # Bonafide
            cnt_r += 1
        elif "FAKE" in path or "Fake" in path:
            if cnt_f >= limit:
                continue
            lbl = 1 # Spoof
            cnt_f += 1
        else:
            continue # Skip if label cannot be determined

        # Append dictionary to itw_audio to match `audio_data` structure from Section 1
        itw_audio.append({
            'audio': ex["audio"]["array"],
            'sr': ex["audio"]["sampling_rate"],
            'label': lbl # Add label to the dict as well
        })
        itw_labels.append(lbl) # Append label to itw_labels for later use

        if cnt_r >= limit and cnt_f >= limit:
            break

    print(f"\nVeri Toplama TamamlandÄ±: {cnt_r} Real + {cnt_f} Fake")

except Exception as e:
    print(f"Hata: {e}")
# ============================================================
# 2. PREPROCESSING & FEATURE EXTRACTION
# ============================================================

if len(itw_audio) > 0:
    print("\nProcessing InTheWild data (Preprocess -> Features)...")

    X_itw_list = []

    # Daha Ã¶nce tanÄ±mladÄ±ÄŸÄ±mÄ±z fonksiyonlarÄ± kullanÄ±yoruz
    for item in tqdm(itw_audio, desc="Extracting Features", unit="sample"):
        # 1. Preprocess (Resample & Pad/Truncate)
        processed_audio = preprocess_audio(item['audio'], item['sr'])

        # 2. Feature Extraction (154-D Robust Features)
        # extract_all_robust_features fonksiyonu Section 3'te tanÄ±mlÄ±ydÄ±
        feats = extract_all_robust_features(processed_audio, sr=CONFIG['sr'])
        X_itw_list.append(feats)

    X_itw = np.array(X_itw_list, dtype=np.float32)
    y_itw = np.array(itw_labels)

    print(f"  Feature Matrix: {X_itw.shape}")

    # ============================================================
    # 3. SCALING (CRITICAL: USE ASVSPOOF SCALER)
    # ============================================================

    print("\nApplying Scaling (Using scaler trained on ASVspoof)...")
    # KayÄ±tlÄ± scaler'Ä± yÃ¼kle (Section 4'te kaydedilmiÅŸti)
    scaler_path = os.path.join(DIRS['models'], 'scaler.pkl')
    scaler = joblib.load(scaler_path)

    # Sadece TRANSFORM yapÄ±yoruz (Fit yok!)
    X_itw_scaled = scaler.transform(X_itw)

    # Tensor'a Ã§evir
    X_itw_t = torch.FloatTensor(X_itw_scaled).to(device)
    y_itw_t = torch.FloatTensor(y_itw).to(device) # Metrik hesabÄ± iÃ§in gerekebilir

    # ============================================================
    # 4. TEST ALL OPTIMIZERS ON ITW DATA
    # ============================================================

    print("\nTesting Optimizers on InTheWild Data (Cross-Dataset)...")

    itw_results = []
    # Tahmin olasÄ±lÄ±klarÄ±nÄ± sonraki Thresholding aÅŸamasÄ± iÃ§in saklayacaÄŸÄ±z
    itw_predictions = {}

    for opt_name in CONFIG['optimizers'].keys():
        print(f"  Evaluting model trained with: {opt_name.upper()}...")

        # Modeli yÃ¼kle
        model_path = os.path.join(DIRS['models'], f'cnn_{opt_name}_model.pth')
        model = ImprovedCNN(input_dim=154).to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()

        # Tahmin yap
        with torch.no_grad():
            logits = model(X_itw_t)
            probs = torch.sigmoid(logits).cpu().numpy().flatten()

        # Standart (0.5) Accuracy ve AUC hesapla
        preds_default = (probs > 0.5).astype(int)
        acc = accuracy_score(y_itw, preds_default)
        auc = roc_auc_score(y_itw, probs)

        itw_results.append({
            'Optimizer': opt_name.upper(),
            'ITW Accuracy (0.5)': acc,
            'ITW AUC': auc
        })

        # OlasÄ±lÄ±klarÄ± kaydet (Bir sonraki aÅŸamada threshold tuning iÃ§in lazÄ±m)
        itw_predictions[opt_name] = probs

    # ============================================================
    # 5. REPORTING
    # ============================================================

    df_itw = pd.DataFrame(itw_results)
    print("\nInTheWild Cross-Dataset Results (Default Threshold 0.5):")
    print(df_itw.to_string(index=False))

    # SonuÃ§larÄ± JSON olarak kaydet
    itw_report_path = save_report(itw_results, 'itw_cross_dataset_results.json')

    # OlasÄ±lÄ±klarÄ± ve gerÃ§ek etiketleri kaydet (Sonraki bÃ¶lÃ¼m iÃ§in)
    np.savez(os.path.join(DIRS['artifacts'], 'itw_predictions.npz'),
             y_true=y_itw,
             probs_adamw=itw_predictions['adamw'], # Ã–zellikle AdamW Ã¶nemli
             probs_adam=itw_predictions['adam'],
             probs_sgd=itw_predictions['sgd'],
             probs_rmsprop=itw_predictions['rmsprop'])

    print(f"\nITW predictions saved for threshold tuning: ./artifacts/itw_predictions.npz")

else:
    print("Veri toplanamadÄ±ÄŸÄ± iÃ§in test atlandÄ±.")

print("\n" + "="*80)
print("SECTION 9 COMPLETE")
print("="*80)

"""
SECTION 10: IN-THE-WILD THRESHOLD OPTIMIZATION
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, accuracy_score, classification_report, roc_auc_score

print("\n" + "="*80)
print("SECTION 10: InTheWild Threshold Optimization & Analysis")
print("="*80)

# 1. TAHMÄ°NLERÄ° YÃœKLE
# ------------------------------------------------------------
try:
    pred_path = os.path.join(DIRS['artifacts'], 'itw_predictions.npz')
    data = np.load(pred_path)

    y_true = data['y_true']
    predictions = {
        'sgd': data['probs_sgd'],
        'adam': data['probs_adam'],
        'adamw': data['probs_adamw'],
        'rmsprop': data['probs_rmsprop']
    }
    print(" Tahminler yÃ¼klendi.")
except Exception as e:
    print(f" Dosya yÃ¼klenemedi: {e}")
    # EÄŸer dosya yoksa ve bir Ã¶nceki adÄ±mdaki deÄŸiÅŸkenler hafÄ±zadaysa onlarÄ± kullan:
    if 'itw_predictions' in locals():
        predictions = itw_predictions
        y_true = y_itw
        print(" Dosya yerine hafÄ±zadaki veriler kullanÄ±lÄ±yor.")

# 2. ANALÄ°Z VE OPTÄ°MÄ°ZASYON FONKSÄ°YONU
# ------------------------------------------------------------
def analyze_and_tune(name, probs, y_true):
    # AUC KontrolÃ¼ (Ters Ã–ÄŸrenme Durumu Ä°Ã§in)
    auc = roc_auc_score(y_true, probs)
    inverted = False

    # EÄŸer AUC < 0.5 ise model ters Ã¶ÄŸrenmiÅŸ demektir (Real'e Fake diyor)
    # Bu durumda olasÄ±lÄ±klarÄ± ters Ã§eviririz (1 - probs)
    if auc < 0.5:
        probs = 1 - probs
        auc = 1 - auc
        inverted = True

    # ROC EÄŸrisi
    fpr, tpr, thresholds = roc_curve(y_true, probs)

    # Youden's J Statistic (En iyi denge noktasÄ±)
    J = tpr - fpr
    best_idx = np.argmax(J)
    best_threshold = thresholds[best_idx]

    # Skorlar
    preds_default = (probs > 0.5).astype(int)
    preds_opt = (probs > best_threshold).astype(int)

    acc_def = accuracy_score(y_true, preds_default)
    acc_opt = accuracy_score(y_true, preds_opt)

    return {
        'name': name,
        'auc': auc,
        'inverted': inverted,
        'best_thresh': best_threshold,
        'acc_default': acc_def,
        'acc_opt': acc_opt,
        'probs': probs, # DÃ¼zeltilmiÅŸ olasÄ±lÄ±klar
        'fpr': fpr,
        'tpr': tpr
    }

# 3. TÃœM OPTIMIZERLARI ANALÄ°Z ET
# ------------------------------------------------------------
results = []
print(f"\n{'OPTIMIZER':<10} | {'DURUM':<10} | {'AUC':<8} | {'ACC (0.5)':<10} | {'ACC (OPT)':<10} | {'BEST THRESH'}")
print("-" * 85)

best_model_result = None
best_auc = -1

for opt_name, probs in predictions.items():
    res = analyze_and_tune(opt_name, probs, y_true)
    results.append(res)

    # En iyi modeli takip et
    if res['auc'] > best_auc:
        best_auc = res['auc']
        best_model_result = res

    status = "TERS" if res['inverted'] else "NORMAL"
    print(f"{opt_name.upper():<10} | {status:<10} | {res['auc']:.4f}   | {res['acc_default']:.4f}     | {res['acc_opt']:.4f}     | {res['best_thresh']:.4f}")

# 4. EN Ä°YÄ° SONUCU DETAYLANDIR
# ------------------------------------------------------------
if best_model_result:
    bm = best_model_result
    print("\n" + "="*80)
    print(f" EN Ä°YÄ° PERFORMANS: {bm['name'].upper()}")
    print("="*80)

    if bm['inverted']:
        print(" DÄ°KKAT: Model sÄ±nÄ±flarÄ± ters Ã¶ÄŸrenmiÅŸ (Inverted).")
        print("   OlasÄ±lÄ±klar (1 - p) iÅŸlemi ile dÃ¼zeltildi.")

    print(f"   InTheWild AUC: {bm['auc']:.4f}")
    print(f"   Accuracy ArtÄ±ÅŸÄ±: %{bm['acc_default']*100:.2f} -> %{bm['acc_opt']*100:.2f}")

    print("\n SÄ±nÄ±flandÄ±rma Raporu (Optimize EdilmiÅŸ):")
    final_preds = (bm['probs'] > bm['best_thresh']).astype(int)
    print(classification_report(y_true, final_preds, target_names=['Real', 'Fake']))

    # 5. GÃ–RSELLEÅžTÄ°RME
    # ------------------------------------------------------------
    plt.figure(figsize=(10, 5))

    # ROC Curve
    plt.subplot(1, 2, 1)
    plt.plot(bm['fpr'], bm['tpr'], label=f"{bm['name'].upper()} (AUC={bm['auc']:.2f})", color='blue', linewidth=2)
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.scatter(bm['fpr'][np.argmax(bm['tpr'] - bm['fpr'])], bm['tpr'][np.argmax(bm['tpr'] - bm['fpr'])],
                color='red', label=f'Best Thresh ({bm["best_thresh"]:.2f})', zorder=5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve (InTheWild)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Histogram
    plt.subplot(1, 2, 2)
    # GerÃ§ek ve Fake olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±
    real_probs = bm['probs'][y_true == 0]
    fake_probs = bm['probs'][y_true == 1]

    plt.hist(real_probs, bins=20, alpha=0.5, label='Real Samples', color='green')
    plt.hist(fake_probs, bins=20, alpha=0.5, label='Fake Samples', color='red')
    plt.axvline(bm['best_thresh'], color='black', linestyle='--', label=f'Threshold {bm["best_thresh"]:.2f}')
    plt.title('Probability Distribution')
    plt.xlabel('Deepfake Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

"""
SECTION 10: IN-THE-WILD THRESHOLD OPTIMIZATION

 AmaÃ§: 4 farklÄ± optimizer'Ä±n tahminlerini birleÅŸtirerek
 tekil hatalarÄ± minimize etmek ve Accuracy'yi artÄ±rmak.
"""

print("\n" + "="*80)
print("SECTION 11: Ensemble Learning (Model BirleÅŸtirme)")
print("="*80)

# 1. TAHMÄ°NLERÄ° HAZIRLA VE DÃœZELT
# ------------------------------------------------------------
ensemble_probs = []
model_weights = []

print(f"{'MODEL':<10} | {'DURUM':<10} | {'AUC':<8} | {'AÄžIRLIK'}")
print("-" * 60)

for opt_name, probs in predictions.items():
    # GeÃ§ici AUC hesabÄ±
    auc = roc_auc_score(y_true, probs)

    # Ters Ã¶ÄŸrenme kontrolÃ¼ ve dÃ¼zeltme
    if auc < 0.5:
        probs_fixed = 1 - probs
        auc_fixed = 1 - auc
        status = "DÃœZELTÄ°LDÄ°"
    else:
        probs_fixed = probs
        auc_fixed = auc
        status = "NORMAL"

    # Modele aÄŸÄ±rlÄ±k ver (AUC ne kadar yÃ¼ksekse o kadar sÃ¶z hakkÄ± olsun)
    # Basit bir yÃ¶ntem: weight = AUC^2 (BaÅŸarÄ±lÄ± modelleri daha Ã§ok Ã¶dÃ¼llendir)
    weight = auc_fixed ** 2

    ensemble_probs.append(probs_fixed * weight)
    model_weights.append(weight)

    print(f"{opt_name.upper():<10} | {status:<10} | {auc_fixed:.4f}   | {weight:.4f}")

# 2. AÄžIRLIKLI ORTALAMA AL (SOFT VOTING)
# ------------------------------------------------------------
# TÃ¼m tahminleri topla ve toplam aÄŸÄ±rlÄ±ÄŸa bÃ¶l
final_ensemble_probs = np.sum(ensemble_probs, axis=0) / np.sum(model_weights)

print("\n Modeller birleÅŸtirildi (Weighted Average Ensemble).")

# 3. ENSEMBLE THRESHOLD OPTIMIZATION
# ------------------------------------------------------------
fpr, tpr, thresholds = roc_curve(y_true, final_ensemble_probs)
J = tpr - fpr
best_idx = np.argmax(J)
best_thresh_ens = thresholds[best_idx]

# Skorlar
preds_default = (final_ensemble_probs > 0.5).astype(int)
preds_opt = (final_ensemble_probs > best_thresh_ens).astype(int)

auc_ens = roc_auc_score(y_true, final_ensemble_probs)
acc_def = accuracy_score(y_true, preds_default)
acc_opt = accuracy_score(y_true, preds_opt)

# 4. SONUÃ‡LARI YAZDIR
# ------------------------------------------------------------
print("\n" + "="*60)
print(f" ENSEMBLE (TOPLULUK) PERFORMANSI")
print("="*60)
print(f" Ensemble AUC:         {auc_ens:.4f}")
print(f" Accuracy (0.5):       %{acc_def*100:.2f}")
print(f" Accuracy (Opt. {best_thresh_ens:.2f}): %{acc_opt*100:.2f}")

# Tekil en iyi model ile kÄ±yaslama
best_single_acc = max([res['acc_opt'] for res in results])
improvement = acc_opt - best_single_acc

if improvement > 0:
    print(f" BAÅžARI: Ensemble, tekil en iyi modelden +%{improvement*100:.2f} daha iyi!")
else:
    print(f"Note: Ensemble, tekil en iyi modelle benzer performans gÃ¶sterdi.")

print("-" * 60)
print("\n SÄ±nÄ±flandÄ±rma Raporu (Ensemble):")
print(classification_report(y_true, preds_opt, target_names=['Real', 'Fake']))

# GÃ¶rselleÅŸtirme
plt.figure(figsize=(6, 5))
cm = confusion_matrix(y_true, preds_opt)
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',
            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.title(f'Ensemble Confusion Matrix\n(AUC: {auc_ens:.2f})')
plt.ylabel('GerÃ§ek')
plt.xlabel('Tahmin')
plt.show()

# ============================================================
# SECTION 12: TEST-TIME AUGMENTATION (TTA)
# ============================================================
# Strateji: Her sese hafif gÃ¼rÃ¼ltÃ¼ ekleyip modele tekrar soruyoruz.
# AmaÃ§: Modelin kararsÄ±z kaldÄ±ÄŸÄ± noktalarÄ± gÃ¼rÃ¼ltÃ¼ ile zorlayÄ±p netleÅŸtirmek.

print("\n" + "="*80)
print("SECTION 12: Test-Time Augmentation (TTA) ile Son VuruÅŸ")
print("="*80)

# TTA iÃ§in gerekli kÃ¼tÃ¼phaneler (Librosa zaten var)
import copy

# ------------------------------------------------------------
# 1. YARDIMCI FONKSÄ°YON: TTA TAHMÄ°NÄ°
# ------------------------------------------------------------
def predict_with_tta(audio, sr, model, scaler, device):
    """
    Bir ses dosyasÄ± iÃ§in 3 farklÄ± versiyon oluÅŸturup tahmin alÄ±r:
    1. Orijinal
    2. Hafif GÃ¼rÃ¼ltÃ¼lÃ¼ (Gaussian Noise)
    3. GenliÄŸi DeÄŸiÅŸtirilmiÅŸ (Amplitude Scaling)
    """
    probs_list = []

    # Versiyonlar
    versions = []

    # V1: Orijinal
    versions.append(audio)

    # V2: GÃ¼rÃ¼ltÃ¼ Ekleme (SNR benzeri etki)
    noise = np.random.randn(len(audio))
    aug_noise = audio + 0.005 * noise # Ã‡ok hafif gÃ¼rÃ¼ltÃ¼
    versions.append(aug_noise)

    # V3: Ses Seviyesi DeÄŸiÅŸimi (0.9x)
    aug_amp = audio * 0.9
    versions.append(aug_amp)

    # Her versiyon iÃ§in tahmin al
    for wav in versions:
        # Preprocess & Feature Extraction
        # Not: preprocess_audio fonksiyonu Section 2'de tanÄ±mlÄ±ydÄ±
        proc_wav = preprocess_audio(wav, sr)
        feats = extract_all_robust_features(proc_wav, sr=16000)

        # Scale
        feats_scaled = scaler.transform(feats.reshape(1, -1))
        feats_t = torch.FloatTensor(feats_scaled).to(device)

        # Predict
        with torch.no_grad():
            logits = model(feats_t)
            prob = torch.sigmoid(logits).cpu().item()
            probs_list.append(prob)

    # 3 tahminin ortalamasÄ±nÄ± dÃ¶ndÃ¼r
    return np.mean(probs_list)

# ------------------------------------------------------------
# 2. TTA Ä°ÅžLEMÄ°NÄ° BAÅžLAT (SADECE ADAMW ve ADAM ÃœZERÄ°NDE)
# ------------------------------------------------------------
# En iyi performans veren iki modeli kullanalÄ±m: AdamW ve Adam
# Ã‡Ã¼nkÃ¼ TTA iÅŸlem gÃ¼cÃ¼ gerektirir, 4 modele yapmak uzun sÃ¼rer.

if 'itw_audio' in locals() and len(itw_audio) > 0:
    print(f"ðŸ”„ TTA Ä°ÅŸlemi BaÅŸlÄ±yor ({len(itw_audio)} Ã¶rnek iÃ§in)...")
    print("   (Bu iÅŸlem her Ã¶rnek iÃ§in 3 kez Ã¶zellik Ã§Ä±karacaÄŸÄ± iÃ§in biraz sÃ¼rebilir)")

    tta_preds_adamw = []
    tta_preds_adam = []

    # Scaler'Ä± yÃ¼kle
    scaler = joblib.load(os.path.join(DIRS['models'], 'scaler.pkl'))

    # Modelleri HazÄ±rla
    model_adamw = ImprovedCNN(input_dim=154).to(device)
    model_adamw.load_state_dict(torch.load(os.path.join(DIRS['models'], 'cnn_adamw_model.pth'), map_location=device))
    model_adamw.eval()

    model_adam = ImprovedCNN(input_dim=154).to(device)
    model_adam.load_state_dict(torch.load(os.path.join(DIRS['models'], 'cnn_adam_model.pth'), map_location=device))
    model_adam.eval()

    # DÃ¶ngÃ¼
    for item in tqdm(itw_audio, desc="TTA Analizi", unit="sample"):
        # AdamW Tahmini (TTA ile)
        p_adamw = predict_with_tta(item['audio'], item['sr'], model_adamw, scaler, device)
        tta_preds_adamw.append(p_adamw)

        # Adam Tahmini (TTA ile)
        p_adam = predict_with_tta(item['audio'], item['sr'], model_adam, scaler, device)
        tta_preds_adam.append(p_adam)

    # Numpy array'e Ã§evir
    probs_adamw_tta = np.array(tta_preds_adamw)
    probs_adam_tta = np.array(tta_preds_adam)

    # ------------------------------------------------------------
    # 3. ENSEMBLE + TTA + OPTIMIZATION
    # ------------------------------------------------------------

    # HatÄ±rlatma: AdamW TERS Ã¶ÄŸrenmiÅŸti, dÃ¼zeltelim
    if roc_auc_score(y_true, probs_adamw_tta) < 0.5:
        probs_adamw_tta = 1 - probs_adamw_tta
        print("â„¹ï¸  AdamW olasÄ±lÄ±klarÄ± dÃ¼zeltildi (Inverted).")

    # Basit Ortalama (En gÃ¼Ã§lÃ¼ iki modelin birleÅŸimi)
    final_tta_probs = (probs_adamw_tta + probs_adam_tta) / 2

    # Threshold Tuning
    fpr, tpr, thresholds = roc_curve(y_true, final_tta_probs)
    J = tpr - fpr
    best_idx = np.argmax(J)
    best_thresh_tta = thresholds[best_idx]

    preds_tta_opt = (final_tta_probs > best_thresh_tta).astype(int)
    acc_tta = accuracy_score(y_true, preds_tta_opt)
    auc_tta = roc_auc_score(y_true, final_tta_probs)

    # ------------------------------------------------------------
    # 4. SONUÃ‡ RAPORU
    # ------------------------------------------------------------
    print("\n" + "="*60)
    print(f"ðŸ”¥ FINAL TTA SONUÃ‡LARI (Ensemble + Augmentation)")
    print("="*60)
    print(f"ðŸ“Š TTA Ensemble AUC:      {auc_tta:.4f}")
    print(f"ðŸ† TTA Accuracy (Opt):    %{acc_tta*100:.2f}")
    print(f"ðŸ“ Best Threshold:        {best_thresh_tta:.4f}")

    # Ã–nceki en iyi skorla kÄ±yas
    # (Ã–nceki en iyi skoru acc_opt deÄŸiÅŸkeninden hatÄ±rlÄ±yoruz veya 0.65 kabul ediyoruz)
    prev_best = 0.65
    print(f"ðŸš€ Ä°yileÅŸtirme (vs %65.0): +{(acc_tta - prev_best)*100:.2f} puan")
    print("-" * 60)

    print("\nðŸ“‹ SÄ±nÄ±flandÄ±rma Raporu (Final):")
    print(classification_report(y_true, preds_tta_opt, target_names=['Real', 'Fake']))

else:
    print("âŒ Veri bulunamadÄ±.")